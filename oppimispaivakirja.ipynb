{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "right-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ilmeisesti on best practice importtaa kaikki aina aluksi\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-consolidation",
   "metadata": {},
   "source": [
    "# Luento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-spanish",
   "metadata": {},
   "source": [
    "### Viikon Joda-opiskelut\n",
    "\n",
    "Viikon luennon katsoin tallenteelta, mutta demoluennon ensimmäisen puoliskon katsoin livenä. Päiväkirjan laatimisessa käytetty materiaali tulee lähinnä luentomateriaaleista, mutta esimerkiksi Jupyterin käyttöönotossa hain apua YouTubesta: https://www.youtube.com/watch?v=DKiI6NfSIe8&t=2s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-wiring",
   "metadata": {},
   "source": [
    "### Datatiede?\n",
    "\n",
    "Datatiede on saanut paljon kritiikkiä siitä, että se on vain tilastotiedettä modernisoidulla nimellä ja datatieteilijä on vain tilastotieteilijä hienommalla titteliä. Asia ei todellisuudessa kuitenkaan ole näin: tilastollisen analyysin lisäksi datatieteilijän tulee hanskata monia muitakin aihealueita, kuten liiketoiminta, ohjelmointi, tietokannat sekä data tehokas viestintä yleisölleen esimerkiksi visualisoinnin kautta. Voidaan siis sanoa, että tilastotiede on osa datatiedettä.\n",
    "\n",
    "CRIPS-DM malli kuvaa mielestäni hyvin datatieteilijän työnkuvaa. Aluksi lähetään siitä, että ymmärretään omaa liiketoimintaa ja dataa. Tämän jälkeen täytyy ymmärtää, että miten ne liittyvät yhteen ja millainen data on ylipäätänsä relevanttia liiketoiminnan kannalta. Tämä data täytyy sen jälkeen ”siivota” kaikesta kyseenomaisessa tapauksessa epärelevantista datasta ja mallintaa siten, että se on helppo viestiä ja helposti ymmärrettävissä. Lopuksi datan perusteella pitää osata tehdä johtopäätöksiä liiketoiminnan tukemiseksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-milton",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Internet ja sen mukana tulleet hullutukset kuten sosiaalinen media ovat mahdollistaneet yritysten liiketoiminnan ainoastaan datan keräämiselle ja myymiselle (Netflixissä on tosi mielenkiintoinen dokkari asiaan liittyen). Yritykset kuten Facebook, keräävät käyttäjistään mahdollisimman paljon dataa ja selvittävät sen avulla, onko kyseinen henkilö esimerkiksi kiinnostunut ostamaan uusia kenkiä. Tämän jälkeen Facebook voi myydä esimerkiksi mainospaikkoja siten, että kenkävalmistajien mainokset näkyvät vain heille, joilla todennäköisesti on intressejä ostaa kengät. Esimerkiksi Google tililtäsi näkee, mitä kaikkea Google sinusta tietää. Netflix dokkaria lainaten, ”datankerääjän todennäköisesti tietävät poliittiset näkemyksesi paremmin kuin sinä itse”.\n",
    "\n",
    "Datan käsittelyn välineiden, kuten tekoälyn kehitys on mahdollistanut myös sen, että tästä datasta voidaan helposti jalostaa tietoa. Myös laskentatehon kasvaminen ja uusien työvälineiden jatkuva keksiminen ja päivittyminen ovat olleet vauhdittamassa datatieteen kehitystä ja kasvua. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-lightweight",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "Edellä kerrotun teorian lisäksi opin myös datan käsittelemistä Jupyterilla ja Pythonilla. Alla olevan demo data on sama, kuin aiemmin linkkaamassa \"Jupyter Notebook Tutorial\"-videossa, ja se löytyy täältä: https://www.kaggle.com/ronitf/heart-disease-uci. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luetaan data ja tallennetaan muuttujaan df\n",
    "original_df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# Kopiodaan data uuteen muuttujaan, jotta ei sörkitä vanhaa\n",
    "df = original_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tulostetaan ensimmäiset 5 riviä\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tulostetaan datatyypit\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-brisbane",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Valitaan ja summataan kolesteroliarvot iän mukaan kolesteroliarvot\n",
    "ages_chols = df.loc[:, [\"age\", \"chol\"]]\n",
    "\n",
    "age_over_50 = ages_chols[ ages_chols[\"age\"] <= 60]\n",
    "age_over_50 = age_over_50[ age_over_50[\"age\"] > 50]\n",
    "\n",
    "age_over_60 = ages_chols[ages_chols[\"age\"] <= 70]\n",
    "age_over_60 = age_over_60[age_over_60[\"age\"] >60]\n",
    "\n",
    "age_over_70 = ages_chols[ ages_chols[\"age\"] > 70 ]\n",
    "\n",
    "chol_sum_over_50 = sum(age_over_50[\"chol\"])\n",
    "chol_sum_over_60 = sum(age_over_60[\"chol\"])\n",
    "chol_sum_over_70 = sum(age_over_70[\"chol\"])\n",
    "\n",
    "# Visualisoidaan data\n",
    "df_plot = pd.DataFrame([[chol_sum_over_50, chol_sum_over_60, chol_sum_over_70]])\n",
    "df_plot.columns = ['Age over 50', 'Age over 60', 'Age over 70']\n",
    "\n",
    "df_plot.plot.bar(figsize=(13,8))\n",
    "plt.legend(loc=2)\n",
    "plt.ylabel('Cholesterol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-temperature",
   "metadata": {},
   "source": [
    "### Viisi oivaillusta tältä viikolta\n",
    "\n",
    "1. Datatiede kattaa tilastoanalyysin lisäksi monia muita osaamisalueita\n",
    "2. Dataa kerätään jatkuvasti ja paljon\n",
    "3. Datan käsittelyn välineet kehittyvät huimaa vaihtua\n",
    "4. Opin käyttämään Jupyteria datan käsittelyn välineenä\n",
    "5. Datan mallintaminen voi mennä pienemmästäkin syntaksivirheestä pieleen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-houston",
   "metadata": {},
   "source": [
    "### Kehitysehdotukset\n",
    "\n",
    "En keksi vielä tältä viikolta kehitysehdotuksia. Haluan esittää siitä kiitokset, että vanhoilla materiaaleilla pystyy suorittamaan kurssia juuri siinä aikataulussa, mikä itselle sopii. Tämä on suuri helpostus minulle aikatauluni suunnittelun suhteen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-cycle",
   "metadata": {},
   "source": [
    "# Luento 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-break",
   "metadata": {},
   "source": [
    "### Viikon Joda-opiskelut\n",
    "Tällä viikolla katsoin sekä luennon, että demon tallenteilta. Päiväkirjan laadin luentomateriaalin avulla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-advance",
   "metadata": {},
   "source": [
    "### Datatieteen prosesista\n",
    "Datatieteen prosessiin kuuluu neljä päävaihetta: tiedon käsittely, analyysi, reflektio ja tulosten viestiminen vastaanottajalle soveltuvassa muodossa. Tästä proseduurista noin 80 % työpanoksesta menee ensimmäiseen vaiheeseen, mikä sisältää datan hankkimisen ja siivoamisen (80/20 -sääntö).  Data voidaan hakea esimerkiksi julkisten rajapintojen kautta, mittauslaitteiden avulla tai tuottamalla se manuaalisesti. Raakadatan perusteella ei usein voi suoraan tehdä analyysiä ja vetää siitä johtopäätöksiä, vaan se täytyy siivota ja formatoida uudestaan. Tämä johtuu siitä, että dataa tuskin on alun perin tuotettu sitä käyttötarkoitusta varten, mihin esimerkiksi tutkija sitä haluaa hyödyntää. Data saattaa usein sisältää myös virheitä (esim. perus kirjoitusvirheitä jne.), jotka saattavan kaataa tai aiheuttaa bugeja sitä käsittelevälle ohjelmistolle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-housing",
   "metadata": {},
   "source": [
    "### Datatieteilijä vs datainsinööri\n",
    "Yksi suurimmista eroista datatieteilijän ja datainsinöörin välillä on työprosessien eroavaisuus. ETL (Extract/Load/Transform) on tarkoitettu datainsinööreille ja DAD (Discover/Access /Distill) on datatieteilijöille. Datainsinöörin tehtävä on keskittyä datan käsittelyyn liittyviin teknisiin ratkaisuihin, kun taas datatieteilijä pyrkii luomaan datasta syvällisempää ymmärrystä ja luoda arvoa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-murray",
   "metadata": {},
   "source": [
    "### Liiketoimintarelevanssi\n",
    "Data-analytiikalla on kyky luoda yritykselle valmiudet suorituskyvyn parantamiseen. Tämä tapahtuu lisäämällä ymmärrystä liiketoimintaprosessisista. Data-analytiikan koostuu seuraavista teemoista:\n",
    "- **Kuvaileva analytiikka**, mitä on tapahtunut?\n",
    "- **Diagnosoiva analytiikka**, miksi se on tapahtunut?\n",
    "- **Ennakoiva analytiikka**, mitä todennäköisesti tulee tapahtumaan?\n",
    "- **Ohjaava analytiikka**, mitä asialle pitäisi tehdä?\n",
    "\n",
    "Vaikka data-analytiikka luo edellytykset liiketoiminnan kehittämiselle, suurin osa yrityksistä ei kuitenkaan sitä ole vielä omaksunut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-software",
   "metadata": {},
   "source": [
    "### Ryömijät ja raapijat\n",
    "Eräs tapa datankeruulle on ryömijöiden ja raapijoiden käyttö. Ryömijä on botti, joka on ohjelmoitu käymään systemaattisesti verkkosivuja läpi. Raapija on taas työkalu, joka kerää dataa verkkosivuilta. Käytännössä ryömijä ja raapija toimivat yhdessä siten, että raapija on ikään kuin ryömijän mukana ja kerää dataa samanaikaisesti, kun ryömijä käy verkkosivuja läpi. Ryömijöitä ja raapijoita käyttäessä tulee ottaa huomioon mahdolliset lailliset esteet, sillä niiden keräämä data ei välttämättä aina ole julkista. Tämä taas vaikuttaa siihen, kuinka kerättyä dataa saa käsitellä ja julkaista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-soundtrack",
   "metadata": {},
   "source": [
    "### Demo\n",
    "Demona toteutan samanlaisen datascraperin Pythonin Scrapy-kirjastoa käyttäen. Otan tähä mallia YouTube-videosta: https://www.youtube.com/watch?v=ogPMCpcgb-E. Huomaa, että demo löytyy kokonaisuudessaan samasta repositoriosta (reddit_scraper.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-restaurant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (4.6.2)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (21.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (5.2.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (0.2.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from scrapy) (3.2.1)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: twisted-iocpsupport~=1.0.0; platform_system == \"Windows\" in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (1.0.1)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.14.3)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anttoni\\miniconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
      "C:\\Users\\Anttoni\\Documents\\Koulu\\Joda\\oppimispaivakirja\\reddit_scraper.py already exists\n"
     ]
    }
   ],
   "source": [
    "# Asennetaan Scraoy, ja generoidaan scraperin skeleton\n",
    "!pip install scrapy\n",
    "!scrapy genspider reddit_scraper reddit.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "permanent-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hakee Redditin langasta r/dogs kaikki .jpg-muotoiset kuvat\n",
    "\n",
    "class RedditScraperSpider(scrapy.Spider):\n",
    "    name = 'reddit_scraper'\n",
    "    allowed_domains = ['reddit.com']\n",
    "    start_urls = ['https://www.reddit.com/r/dogs/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        links = response.xpath(\"//img/@src\")\n",
    "        html = \"\"\n",
    "\n",
    "        for link in links:\n",
    "            url = link.get()\n",
    "\n",
    "            if any(extension in url for extension in [\".jpg\"]):\n",
    "                html += \"\"\"<a href=\"{url}\" target=\"_blank\"><img src=\"{url}\" height=\"25%\" width=\"25%\"/><a/>\"\"\".format(url=url)\n",
    "\n",
    "                with open(\"dogs.html\", \"a\") as page:\n",
    "                    page.write(html)\n",
    "                    page.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acute-baltimore",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-19 10:53:13 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-03-19 10:53:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1j  16 Feb 2021), cryptography 3.2.1, Platform Windows-10-10.0.19041-SP0\n",
      "2021-03-19 10:53:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2021-03-19 10:53:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2021-03-19 10:53:13 [scrapy.extensions.telnet] INFO: Telnet Password: 0717d357272457e8\n",
      "2021-03-19 10:53:13 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-03-19 10:53:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-03-19 10:53:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-03-19 10:53:13 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-03-19 10:53:13 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-03-19 10:53:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-03-19 10:53:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-03-19 10:53:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.reddit.com/r/dogs/> (referer: None)\n",
      "2021-03-19 10:53:16 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-03-19 10:53:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 225,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 173697,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 3.306897,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 3, 19, 8, 53, 16, 636201),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2021, 3, 19, 8, 53, 13, 329304)}\n",
      "2021-03-19 10:53:16 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Ajetaan scripti syöttämällä seuraava käsky komentoriville\n",
    "!scrapy runspider reddit_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-tyler",
   "metadata": {},
   "source": [
    "### Viisi oivaillusta tältä viikolta\n",
    "\n",
    "1. 80/20 -sääntö: datan keräämiseen ja siivoamiseen menee usein kauemmin, kuin sen analysoimiseen\n",
    "2. Datainsinööri hoitaa datankäsittelyn teknisen toteutuksen ja datatieteilijä luo sillä arvoa\n",
    "3. Data-analytiikan neljä teemaa\n",
    "4. Ryömijän ja raapijan yhteistoiminta\n",
    "5. Ryömijän ja raapijan toteuttaminen Pythonin Scrapy-kirjastolla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-movement",
   "metadata": {},
   "source": [
    "### Kehitysehdotukset\n",
    "\n",
    "En keksi vieläkään kehitysehdotuksia. Kurssi on ollut erittäin hyvin järjestetty (tähän mennessä), ja mielestäni monet kurssinvetäjät esim. tietotekniikan puolelta voisivat ottaa tästä mallia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-input",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
